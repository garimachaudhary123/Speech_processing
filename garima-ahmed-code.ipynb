{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import subprocess\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from keras.layers import Conv1D\n",
    "import keras\n",
    "import joblib\n",
    "import librosa\n",
    "\n",
    "data_directory = \"Speech-processing\\\\Keras-speaker-recognisition\"\n",
    "audio_folder = \"audio\"\n",
    "noise_folder = \"noise\"\n",
    "\n",
    "audio_path = os.path.join(data_directory, audio_folder)\n",
    "noise_path = os.path.join(data_directory, noise_folder)\n",
    "\n",
    "valid_split = 0.1\n",
    "\n",
    "shuffle_seed = 43\n",
    "\n",
    "# Sample rate, also known as sampling rate, refers to the number of samples of audio that are taken per second \n",
    "# when digitizing or recording an analog audio signal. It is measured in Hertz (Hz) and \n",
    "# typically expressed in kilohertz (kHz) or megahertz (MHz).\n",
    "\n",
    "sample_rate = 16000\n",
    "\n",
    "scale = 0.5\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(data_directory):\n",
    "    if os.path.isdir(os.path.join(data_directory, folder)):\n",
    "        if folder in [audio_folder, noise_folder]:\n",
    "            \n",
    "            continue\n",
    "        elif folder in [\"other\", \"_background_noise_\"]:\n",
    "            \n",
    "            shutil.move(\n",
    "                os.path.join(data_directory, folder),\n",
    "                os.path.join(noise_path, folder),\n",
    "            )\n",
    "        else:\n",
    "            shutil.move(\n",
    "                os.path.join(data_directory, folder),\n",
    "                os.path.join(audio_path, folder),\n",
    "            )\n",
    "\n",
    "noise_paths = []\n",
    "for subdir in os.listdir(noise_path):\n",
    "    subdir_path = Path(noise_path) / subdir\n",
    "    if os.path.isdir(subdir_path):\n",
    "        noise_paths += [\n",
    "            os.path.join(subdir_path, filepath)\n",
    "            for filepath in os.listdir(subdir_path)\n",
    "            if filepath.endswith(\".wav\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\AppData\\Local\\Temp\\ipykernel_35168\\3626655206.py:15: FutureWarning: Pass orig_sr=16000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio_resampled = librosa.resample(audio_file, sr, 16000)\n",
      "C:\\Users\\Atharva\\AppData\\Local\\Temp\\ipykernel_35168\\3626655206.py:15: FutureWarning: Pass orig_sr=22050, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio_resampled = librosa.resample(audio_file, sr, 16000)\n",
      "C:\\Users\\Atharva\\AppData\\Local\\Temp\\ipykernel_35168\\3626655206.py:15: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio_resampled = librosa.resample(audio_file, sr, 16000)\n"
     ]
    }
   ],
   "source": [
    "command = (\n",
    "    \"for dir in `ls -1 \" + noise_path + \"`; do \"\n",
    "    \"for file in `ls -1 \" + noise_path + \"/$dir/*.wav`; do \"\n",
    "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
    "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
    "    \"if [ $sample_rate -ne 16000 ]; then \"\n",
    "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
    "    \"-i $file -ar 16000 temp.wav; \"\n",
    "    \"mv temp.wav $file; \"\n",
    "    \"fi; done; done\"\n",
    ")\n",
    "\n",
    "for path in noise_paths:\n",
    "    audio_file, sr = librosa.load(path, sr=None)\n",
    "    audio_resampled = librosa.resample(audio_file, sr, 16000)\n",
    "    sf.write(path, audio_resampled, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(command)\n",
    "\n",
    "def load_noise_sample(path):\n",
    "    sample, sampling_rate = tf.audio.decode_wav(\n",
    "        tf.io.read_file(path), desired_channels=1\n",
    "    )\n",
    "    if sampling_rate == sample_rate:\n",
    "        slices = int(sample.shape[0] / sample_rate)\n",
    "        sample = tf.split(sample[: slices * sample_rate], slices)\n",
    "        return sample\n",
    "    else:\n",
    "        print(\"Sampling rate for\",path, \"is incorrect\")\n",
    "        return None\n",
    "\n",
    "\n",
    "noises = []\n",
    "for path in noise_paths:\n",
    "    sample = load_noise_sample(path)\n",
    "    if sample:\n",
    "        noises.extend(sample)\n",
    "noises = tf.stack(noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(354, 16000, 1), dtype=float32, numpy=\n",
       "array([[[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        ...,\n",
       "        [-0.00747681],\n",
       "        [ 0.00854492],\n",
       "        [ 0.03442383]],\n",
       "\n",
       "       [[ 0.02334595],\n",
       "        [ 0.02230835],\n",
       "        [ 0.03738403],\n",
       "        ...,\n",
       "        [ 0.04589844],\n",
       "        [ 0.01644897],\n",
       "        [-0.0062561 ]],\n",
       "\n",
       "       [[ 0.02755737],\n",
       "        [ 0.00061035],\n",
       "        [-0.02462769],\n",
       "        ...,\n",
       "        [ 0.03149414],\n",
       "        [ 0.00411987],\n",
       "        [ 0.02502441]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.17086792],\n",
       "        [-0.01544189],\n",
       "        [ 0.01107788],\n",
       "        ...,\n",
       "        [-0.01834106],\n",
       "        [-0.0920105 ],\n",
       "        [-0.04959106]],\n",
       "\n",
       "       [[ 0.02841187],\n",
       "        [ 0.0244751 ],\n",
       "        [-0.05978394],\n",
       "        ...,\n",
       "        [-0.11184692],\n",
       "        [-0.06658936],\n",
       "        [ 0.09091187]],\n",
       "\n",
       "       [[-0.06607056],\n",
       "        [ 0.03790283],\n",
       "        [ 0.00744629],\n",
       "        ...,\n",
       "        [ 0.00509644],\n",
       "        [-0.07342529],\n",
       "        [ 0.10510254]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, sample_rate)\n",
    "    return audio\n",
    "\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "def add_noise(audio, noises=None, scale=0.5):\n",
    "    if noises is not None:\n",
    "        tf_rnd = tf.random.uniform(\n",
    "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
    "        )\n",
    "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
    "\n",
    "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
    "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
    "\n",
    "        audio = audio + noise * prop * scale\n",
    "\n",
    "    return audio\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']\n",
      "Speaker: Benjamin_Netanyau\n",
      "Speaker: Jens_Stoltenberg\n",
      "Speaker: Julia_Gillard\n",
      "Speaker: Magaret_Tarcher\n",
      "Speaker: Nelson_Mandela\n"
     ]
    }
   ],
   "source": [
    "class_names = os.listdir(audio_path)\n",
    "print(class_names,)\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Speaker:\",(name))\n",
    "    dir_path = Path(audio_path) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to generate random data\n",
    "rng = np.random.RandomState(shuffle_seed)\n",
    "rng.shuffle(audio_paths)\n",
    "rng = np.random.RandomState(shuffle_seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Split into training and validation\n",
    "num_val_samples = int(valid_split * len(audio_paths))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "\n",
    "valid_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_labels = labels[-num_val_samples:]\n",
    "\n",
    "# Create datasets, one for training and the other for validation\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 16000, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the training set\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (add_noise(x, noises, scale=scale), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 8000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 8000, 128)    512         ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 8000, 128)    0           ['conv1d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 8000, 128)    49280       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 8000, 128)    0           ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 8000, 128)    49280       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 8000, 128)    256         ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 8000, 128)    0           ['conv1d_35[0][0]',              \n",
      "                                                                  'conv1d_32[0][0]']              \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 8000, 128)    0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 4000, 128)   0           ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling1d_1 (AveragePo  (None, 1333, 128)   0           ['max_pooling1d_9[0][0]']        \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 170624)       0           ['average_pooling1d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          43680000    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 5)            645         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 43,812,869\n",
      "Trainable params: 43,812,869\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "def residual_block(x, filters, conv_num = 3, activation = \"relu\"):\n",
    "    s = layers.Conv1D(filters, 1, padding = \"same\")(x)\n",
    "    \n",
    "    for i in range(conv_num - 1):\n",
    "        x = layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
    "    x = layers.Add()([x, s])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    \n",
    "    return layers.MaxPool1D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape = input_shape, name = \"input\")\n",
    "    \n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(inputs, 32, 2)\n",
    "    x = residual_block(inputs, 64, 3)\n",
    "    x = residual_block(inputs, 128, 3)\n",
    "    x = residual_block(inputs, 128, 3)\n",
    "    x = layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation = \"softmax\", name = \"output\")(x)\n",
    "    \n",
    "    return keras.models.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model = build_model((sample_rate // 2, 1), len(class_names))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "model_save_filename = \"model.h5\"\n",
    "\n",
    "earlystopping_cb = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "mdlcheckpoint_cb = callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "53/53 [==============================] - 197s 4s/step - loss: 2.4096 - accuracy: 0.7000 - val_loss: 0.2334 - val_accuracy: 0.9267\n",
      "Epoch 2/15\n",
      "53/53 [==============================] - 195s 4s/step - loss: 0.1703 - accuracy: 0.9363 - val_loss: 0.1395 - val_accuracy: 0.9453\n",
      "Epoch 3/15\n",
      "53/53 [==============================] - 197s 4s/step - loss: 0.1484 - accuracy: 0.9412 - val_loss: 0.1194 - val_accuracy: 0.9587\n",
      "Epoch 4/15\n",
      "53/53 [==============================] - 194s 4s/step - loss: 0.0876 - accuracy: 0.9693 - val_loss: 0.0881 - val_accuracy: 0.9680\n",
      "Epoch 5/15\n",
      "53/53 [==============================] - 191s 4s/step - loss: 0.1203 - accuracy: 0.9570 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "Epoch 6/15\n",
      "53/53 [==============================] - 189s 4s/step - loss: 0.0746 - accuracy: 0.9747 - val_loss: 0.0862 - val_accuracy: 0.9680\n",
      "Epoch 7/15\n",
      "53/53 [==============================] - 195s 4s/step - loss: 0.0518 - accuracy: 0.9815 - val_loss: 0.0784 - val_accuracy: 0.9747\n",
      "Epoch 8/15\n",
      "53/53 [==============================] - 198s 4s/step - loss: 0.0638 - accuracy: 0.9794 - val_loss: 0.0695 - val_accuracy: 0.9827\n",
      "Epoch 9/15\n",
      "53/53 [==============================] - 201s 4s/step - loss: 0.0717 - accuracy: 0.9748 - val_loss: 0.1113 - val_accuracy: 0.9653\n",
      "Epoch 10/15\n",
      "53/53 [==============================] - 201s 4s/step - loss: 0.0435 - accuracy: 0.9840 - val_loss: 0.0761 - val_accuracy: 0.9853\n",
      "Epoch 11/15\n",
      "53/53 [==============================] - 200s 4s/step - loss: 0.0612 - accuracy: 0.9809 - val_loss: 0.0922 - val_accuracy: 0.9760\n",
      "Epoch 12/15\n",
      "53/53 [==============================] - 199s 4s/step - loss: 0.0327 - accuracy: 0.9877 - val_loss: 0.0901 - val_accuracy: 0.9773\n",
      "Epoch 13/15\n",
      "53/53 [==============================] - 196s 4s/step - loss: 0.0357 - accuracy: 0.9880 - val_loss: 0.1108 - val_accuracy: 0.9760\n",
      "Epoch 14/15\n",
      "53/53 [==============================] - 192s 4s/step - loss: 0.0283 - accuracy: 0.9899 - val_loss: 0.0556 - val_accuracy: 0.9853\n",
      "Epoch 15/15\n",
      "53/53 [==============================] - 196s 4s/step - loss: 0.0359 - accuracy: 0.9874 - val_loss: 0.0527 - val_accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 7s 271ms/step - loss: 0.0527 - accuracy: 0.9787\n",
      "Accuracy of model: [0.05274485424160957, 0.9786666631698608]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of model:\", model.evaluate(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c6e312d-0e46-41e9-a5fe-a9c5d357c36d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c6e312d-0e46-41e9-a5fe-a9c5d357c36d/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['garima-ahmed-model.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'garima-ahmed-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 290ms/step\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n",
      "Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n",
      "Welcome\n",
      "The speaker is Jens_Stoltenberg\n",
      "Speaker:\u001b[92m Nelson_Mandela\u001b[0m\tPredicted:\u001b[92m Nelson_Mandela\u001b[0m\n",
      "Welcome\n",
      "The speaker is Nelson_Mandela\n",
      "Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n",
      "Welcome\n",
      "The speaker is Benjamin_Netanyau\n",
      "Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n",
      "Welcome\n",
      "The speaker is Benjamin_Netanyau\n",
      "Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n",
      "Welcome\n",
      "The speaker is Jens_Stoltenberg\n",
      "Speaker:\u001b[92m Nelson_Mandela\u001b[0m\tPredicted:\u001b[92m Nelson_Mandela\u001b[0m\n",
      "Welcome\n",
      "The speaker is Nelson_Mandela\n",
      "Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n",
      "Welcome\n",
      "The speaker is Benjamin_Netanyau\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_TO_DISPLAY = 10\n",
    "\n",
    "test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "test_ds = test_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n",
    "\n",
    "for audios, labels in test_ds.take(1):\n",
    "    ffts = audio_to_fft(audios)\n",
    "    y_pred = model.predict(ffts)\n",
    "    rnd = np.random.randint(0, batch_size, SAMPLES_TO_DISPLAY)\n",
    "    audios = audios.numpy()[rnd, :, :]\n",
    "    labels = labels.numpy()[rnd]\n",
    "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
    "\n",
    "    for index in range(SAMPLES_TO_DISPLAY):\n",
    "        print(\n",
    "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
    "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "                class_names[labels[index]],\n",
    "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "                class_names[y_pred[index]],\n",
    "            )\n",
    "        )\n",
    "        if labels[index] ==y_pred[index]:\n",
    "            print(\"Welcome\")\n",
    "        else:\n",
    "            print(\"Sorry\")\n",
    "        print(\"The speaker is\" if labels[index] == y_pred[index] else \"\", class_names[y_pred[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataset(audio_paths):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    return tf.data.Dataset.zip((path_ds))\n",
    "\n",
    "def predict(path, labels):\n",
    "    test = paths_and_labels_to_dataset(path, labels)\n",
    "\n",
    "\n",
    "    test = test.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
    "    batch_size\n",
    "    )\n",
    "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "    test = test.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n",
    "\n",
    "    for audios, labels in test.take(1):\n",
    "        ffts = audio_to_fft(audios)\n",
    "        y_pred = model.predict(ffts)\n",
    "        rnd = np.random.randint(0, 1, 1)\n",
    "        audios = audios.numpy()[rnd, :]\n",
    "        labels = labels.numpy()[rnd]\n",
    "        y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
    "\n",
    "    for index in range(1):\n",
    "            # print(\n",
    "            # \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
    "            # \"[92m\",y_pred[index],\n",
    "            #     \"[92m\", y_pred[index]\n",
    "            #     )\n",
    "            # )\n",
    "            \n",
    "            print(\"Speaker Predicted:\",class_names[y_pred[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "Speaker Predicted: Benjamin_Netanyau\n"
     ]
    }
   ],
   "source": [
    "path = [\"audio\\\\Benjamin_Netanyau\\\\25.wav\"]\n",
    "labels = [\"unknown\"]\n",
    "try:\n",
    "    predict(path, labels)\n",
    "except:\n",
    "    print(\"Error! Check if the file correctly passed or not!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
